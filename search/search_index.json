{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the tutorial: mapping burned area with Deep Learning","text":""},{"location":"#about-the-tutorial","title":"About the tutorial","text":"<p>This tutorial is an effort to provide processing code strings for the purpose of using Deep Learning architectures for land cover disturbance mapping, combining Google Earth Engine and Google Colab. This material essentially focuses on mapping burned areas at a regional scale, so that it can be replicated for local or larger scale work.</p>"},{"location":"#wildfire","title":"Wildfire","text":"<p>Human actions are driving significant changes in land cover, intensifying climate change challenges and posing critical threats to communities and ecosystems. These changes arise from various processes and drivers, operating at different scales. It is crucial to capture these diverse processes explicitly in space and time to support scientific and societal applications that go beyond mere geographic representation of disturbances. Wildland fires offer a unique chance to study the role of abrupt land cover change, gradual recovery, and the establishment of new equilibria. Satellite remote sensing provides valuable data on fire location, magnitude, and recovery.</p> <p>For future citations of this work, please use:</p>"},{"location":"#some-sentinel-2-burned-areas-patches","title":"Some Sentinel-2 burned areas patches","text":""},{"location":"Downloading%20patches/","title":"01-Downloading patches from Google Earth Engine","text":"<p>Google Earth Engine (GEE) is a geospatial processing service. GEE allows users to run algorithms on georeferenced imagery and vectors stored on Google's infrastructure. The GEE API provides a library of functions which may be applied to data for display and analysis.With Earth Engine, you can perform geospatial processing at scale, powered by Google Cloud Platform. The purpose of Earth Engine is to:</p> <ol> <li>Provide an interactive platform for geospatial algorithm development at scale</li> <li>Enable high-impact, data-driven science</li> <li>Make substantive progress on global challenges that involve large geospatial datasets</li> </ol>"},{"location":"Downloading%20patches/#01-libraries-to-be-used","title":"01. Libraries to be used:","text":"<pre><code>import ee\nimport geemap as emap\n</code></pre>"},{"location":"Downloading%20patches/#02-authentication-and-initialization","title":"02. Authentication and Initialization","text":"<p>Prior to using the Earth Engine Python client library, you need to authenticate and use the resultant credentials to initialize the Python client. To initialize, you will need to provide a project that you own, or have permissions to use. This project will be used for running all Earth Engine operations:</p> <pre><code>ee.Authenticate()\nee.Initialize(project='ee-geoyons')\n</code></pre> <p>See the authentication guide for troubleshooting and to learn more about authentication modes and Cloud projects.</p> <p>Selecting a specific basemap as follow:</p> <pre><code>Map = emap.Map()\nMap.add_basemap(\"SATELLITE\")\n</code></pre>"},{"location":"Downloading%20patches/#03-downloading-patches-from-gee-and-python","title":"03. Downloading patches from GEE and Python","text":"<p>Patches for Deep Learning architectures are crucial for land cover disturbance mapping, including burned area mapping. A good option is to download from GEE using the Python API or JavaScript, whichever you are comfortable with. In this tutorial, the Python API will be used to stay within the Google Colab environment.</p> <p>A specific tile with 50x50km (i.e., 25 million pixels) from Portugal during the 2017 year will be downloaded. Labeling data known as ground truth for the same tile will be downloaded as well.</p> <p>Let's work on coding.</p> <pre><code># specific tile to be used\ntile = ee.List([17])\n\n# tiles for Portugal\nport_tiles = ee.FeatureCollection(\"projects/ee-geoyons/assets/tiles_portugal_50km\")\n# labeling data - burned area during the 2017 year\nlabel = ee.Image(\"projects/ee-geoyons/assets/raster_ardida_2017\")\n# selecting tile 17\nfeatures = port_tiles.filter(ee.Filter.inList('id', tile));\n\n# define a collection\ncolS2 = 'COPERNICUS/S2_SR_HARMONIZED'\n\n# NBR function\ndef addNBR (image):\n  nbr = image.normalizedDifference(['B8', 'B12']).rename(\"NBR\");\n  return image.addBands(nbr);\n\n# masking clouds\ndef maskSQA(image):\n  qa = image.select('QA60')\n  opaque_cloud = (1 &lt;&lt; 10);\n  cirrus_cloud = (1 &lt;&lt; 11);\n  mask = qa.bitwiseAnd(opaque_cloud).eq(0).And(qa.bitwiseAnd(cirrus_cloud).eq(0));\n  return image.updateMask(mask);\n\ncloud_cover = 10;\n\nbnames = ['B2', 'B3', 'B4', 'B5','B6','B7','B8','B8A','B11','B12']\n\n### ********************************* AFTER THE EVENT ******************************\n# dates AFTER the fire event\nstart_date_after = '2017-09-30';\nend_date_after = '2017-12-30';\n\n# base satellite image as input for burned area\nimg2017 = ee.ImageCollection(colS2)\\\n                .filterBounds(features)\\\n                .filterDate(start_date_after, end_date_after)\\\n                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_cover))\\\n                .map(maskSQA)\\\n                .select(bnames)\\\n                .reduce(ee.Reducer.median())\n\n# NBR collection computed\nnbrColl = ee.ImageCollection(colS2)\\\n                .filterBounds(features)\\\n                .filterDate(start_date_after, end_date_after)\\\n                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', cloud_cover))\\\n                .map(maskSQA)\\\n                .select(bnames)\\\n                .map(addNBR)\\\n                .reduce(ee.Reducer.min());\n\n# NBR after the event\nnbr_after = nbrColl.select('NBR_min');\n\n### ********************************* BEFORE THE EVENT ******************************\n\n# dates BEFORE the fire event\nstart_date_before = '2020-09-30'; # '2017-04-01';\nend_date_before = '2020-12-30'; # '2017-06-15'\n\n# NBR collection computed\nnbrColl2 = ee.ImageCollection(colS2)\\\n                .filterBounds(features)\\\n                .filterDate(start_date_before, end_date_before)\\\n                .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 90))\\\n                .map(maskSQA)\\\n                .select(bnames)\\\n                .map(addNBR)\\\n                .reduce(ee.Reducer.mean());\n\n# NBR before the event\nnbr_before = nbrColl2.select('NBR_mean');\n# dNBR \nnbr_diff = nbr_after.subtract(nbr_before);\n</code></pre> <p>GEE has some visualization parameters for the <code>Map.addLayer()</code> function in order to enhance the image histogram values.</p> <p></p> <p>Courtesy: Qiusheng Wu</p> <pre><code># params for base image visualization\nvizParams = {\n  \"bands\": ['B12_median', 'B8_median', 'B4_median'],\n  \"min\": 0,\n  \"max\": 3500,\n  \"gamma\": [0.95, 1.1, 1]\n  };\n\n# params for NBR index\nparams_nbr = {\n  \"min\": -0.4,\n  \"max\": 0.6,\n  \"palette\": ['FFFFFF','CC9966','CC9900','996600', '33CC00', '009900','006600']};\n\nMap.centerObject(features, 10)\nMap.addLayer(features, {},'Portugal tiles', True);\nMap.addLayer(img2017.clip(features), vizParams, 'S2 - Post-fire event 2017', True);\nMap.addLayer(nbr_before.select('NBR_mean').clip(features), params_nbr, 'NBR before', True);\nMap.addLayer(nbr_after.clip(features).select('NBR_min'), params_nbr, 'NBR after', True);\nMap.addLayer(nbr_diff.clip(features), {'min': -0.9, 'max': 0.9}, 'NBR Difference (dNBR)', True);\nMap\n</code></pre> <p></p>"},{"location":"Downloading%20patches/#04-downloading-and-exporting-images","title":"04. Downloading and exporting images","text":"<p>When you export an image, the data are ordered as channels, height, width (CHW). The export may be split into multiple TFRecord files with each file containing one or more patches of size <code>patchSize</code>, which is user specified in the export. The size of the files in bytes is user specified in the <code>maxFileSize</code> parameter.</p>"},{"location":"Downloading%20patches/#formatoptions","title":"formatOptions","text":"<p>The <code>patchSize</code>, <code>maxFileSize</code>, and <code>kernelSize</code> parameters are passed to the ee.Export (JavaScript) or <code>ee.batch.Export</code> (Python) call through a <code>formatOptions</code> dictionary, where keys are the names of additional parameters passed to <code>Export</code>. Possible <code>formatOptions</code> for an image exported to TFRecord format are on the official GEE documentation page.</p> <p>We generated 512x512 pixel patches for both the 11 input variables (10 S2 bands + 1 dNBR) and the labeling data. Bands of Sentinel-2 used: - B2 (Blue) - B3 (Green) - B4 (Red) - B5 (Vegetation red edge 1) - B6 (Vegetation red edge 2) - B7 (Vegetation red edge 3) - B8 (Near infrared) - B8A (Vegetation red edge 4) - B11 (SWIR1) - B12 (SWIR2) - dNBR (NBR difference) - label (Labeling data)</p> <pre><code># formatOptions\nformatOptions = {\n  'patchDimensions': [512, 512], #512*512\n  'maxFileSize': 104857600,\n  'compressed': True\n}\n\n# exporting patches to Google Drive\nee.batch.Export.image({\n  \"image\": img2017.clip(features).double(),\n  \"description\": 'PatchesExport',\n  \"fileNamePrefix\": 'Port_tile17',\n  \"scale\": 10, # spatial resolution\n  \"folder\": 'Portugal_patches_512-512_tile17',\n  \"fileFormat\": 'TFRecord',\n  \"region\": features,\n  \"formatOptions\": formatOptions,\n  \"maxPixels\": 1e+13,\n})\n</code></pre>"},{"location":"Predictions/","title":"04. Predicting large images","text":""},{"location":"Predictions/#01-key-variables","title":"01. key variables","text":"<p>Let's first save some key variables that will be used for prediction and for reconstructing the whole image in a single patch.</p> <pre><code># total patches\ntotal_patches = mixer['totalPatches'] # 108\n# patches per row\nnum_patches_rows = mixer['patchesPerRow'] # 12\n# patches per col\nnum_patches_cols = total_patches // num_patches_rows # 9\n# path size\npatch_size = 512\n# number of bands\nnbands = 11\n# Patches by rows and cols (integer)\nprint(f'number of patches in rows: {num_patches_rows}\\nnumber of patches in cols: {num_patches_cols}')\n</code></pre>"},{"location":"Predictions/#02-n-dimensional-tensor","title":"02. N-dimensional Tensor","text":"<p>In this step, a 4-dimensional tensor of <code>X</code> will be reshaped to a 6-dimensional tensor, which means <code>patches per col</code>, <code>patches per row</code>, <code>height of 1</code>, <code>path size</code>, <code>patch size</code>, <code>number of bands</code>. Regarding the labelind data, it will be reshape to a 4-dimensional tensor, i.e., <code>patches per col</code>, <code>patches per row</code>, <code>path size</code> and <code>patch size</code>.</p> <pre><code># reshape the image patches\npatches = np.reshape(X, (num_patches_cols,\n                         num_patches_rows,\n                         patch_size,\n                         patch_size,\n                         nbands)) # 9, 12, 512,512,11\n\npatches = np.expand_dims(patches, axis = 2)\nprint(\"Patches array shape is: \", patches.shape)\n\n# reshape the labeling data patches\npatches_labels = np.reshape(y, (num_patches_cols,\n                                num_patches_rows,\n                                patch_size,\n                                patch_size)) # 9, 12, 512,512\n\nprint(\"Patches labeling shape is: \", patches_labels.shape)\n</code></pre>"},{"location":"Predictions/#03-predicting-the-whole-image-tile-17","title":"03. Predicting the whole image: tile 17","text":"<p>In this step, patches as tensor will be used for prediction, then the predicted patches will be saved as a 4-dimensional tensor with <code>patches per col</code>, <code>patches per row</code>, <code>path size</code> and <code>patch size</code>.</p> <pre><code># Apply the trained model on large image, patch by patch\npredicted_patches = []\nfor i in range(num_patches_cols):\n    for j in range(num_patches_rows):\n        print(\"Now predicting on patch\", i,j)\n\n        single_patch = patches[i,j,0,:,:,:]\n\n        single_patch_input = np.expand_dims(single_patch, axis = 0)\n        single_patch_prediction = (model.predict(single_patch_input))\n        single_patch_predicted_img = np.argmax(single_patch_prediction, axis = 3)[0,:,:]\n\n        predicted_patches.append(single_patch_predicted_img)\n\npredicted_patches = np.array(predicted_patches)\n\npredicted_patches = np.reshape(predicted_patches, (num_patches_cols, num_patches_rows, patch_size, patch_size)) # 9, 12, 512, 512\n</code></pre> <p>Verifying the new shape of the predicted patches obtained using the u-net architecture:</p> <pre><code>print(f'number of patches in rows: {predicted_patches.shape[0]}\\nnumber of patches in cols: {predicted_patches.shape[1]}')\nprint(f'shape of patches: {predicted_patches.shape[2]}, {predicted_patches.shape[3]}')\nprint(f'shape of patches: {predicted_patches.shape}')\n</code></pre>"},{"location":"Predictions/#04-reconstructing-the-whole-image-in-a-single-patch","title":"04. Reconstructing the whole image in a single patch","text":"<p>Reconstructing patches into a single patch is crucial to extract statistical metrics from the obtained predictions. This final step can be a challenge if you are working with large areas of land, which implies a large volume of patches to process. Furthermore, this step is an additional challenge if the patches are downloaded from GEE because there will be some columns and rows that will have to be filled in during the patch reconstruction process. This is because your study area will not exactly fit the dimensions of the patches to be downloaded.</p> <p>Keep this in mind when you need to reconstruct the patches and then download them in some format such as \".TIF\", for example.</p> <pre><code># reconstructing the whole image and labeling data\nreconstructed_pred = unpatchify(predicted_patches, (num_patches_cols*patch_size, num_patches_rows*patch_size))\nreconstructed_image = unpatchify(patches, (num_patches_cols*patch_size, num_patches_rows*patch_size, nbands))\nreconstructed_labels = unpatchify(patches_labels, (num_patches_cols*patch_size, num_patches_rows*patch_size))\n</code></pre> <p>Visualizing the reconstructed image, predictions and labeling data in a single patch.</p> <pre><code># histogram with percentiles\ndef hist_percentile(arr_rgb):\n  p10 = np.nanpercentile(arr_rgb, 10) # percentile10\n  p90 = np.nanpercentile(arr_rgb, 90) # percentile90\n  clipped_arr = np.clip(arr_rgb, p10, p90)\n  arr_rgb_norm = (clipped_arr - p10)/(p90 - p10)\n  return arr_rgb_norm\n\norig_map = plt.colormaps['Greys']\nreversed_map = orig_map.reversed()\nimg = reconstructed_image\n\nfig, ax = plt.subplots(1, 3, figsize = (13,8))\n\nrgb_patch = hist_percentile(np.dstack((img[:,:,9], img[:,:,6], img[:,:,2])))\nax[0].imshow(rgb_patch)\nax[0].set_title('Sentinel-2')\nax[1].imshow(reconstructed_labels, cmap = reversed_map)\nax[1].set_title('Labeling data')\nax[2].imshow(reconstructed_pred, cmap = reversed_map)\nax[2].set_title('Prediction on the whole image')\nplt.show()\n</code></pre> <p></p>"},{"location":"Preprocessing%20patches/","title":"02-Preprocessing patches for burned area mapping","text":"<p>In this step patches downloaded from Google Earth Engine (GEE) will be preprocessed as input for applying the U-net architecture in order to map burned areas. We will kick off by reading TFRecod format from Google Drive, and finish with some patches visualizations using the <code>scikit-eo</code> Python package.</p>"},{"location":"Preprocessing%20patches/#01-libraries-to-be-installed","title":"01. Libraries to be installed:","text":"<pre><code>!pip install scikeo\n</code></pre>"},{"location":"Preprocessing%20patches/#02-libraries-to-be-used","title":"02. Libraries to be used:","text":"<pre><code>import json\nimport numpy as np\nfrom scikeo.plot import plotRGB\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom pathlib import Path\nfrom google.colab import drive\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n</code></pre> <p>Count the number of cores in a computer:</p> <pre><code>import multiprocessing\nmultiprocessing.cpu_count()\n</code></pre> <p>Connecting to Google Drive: This step is very importante considering that the downloaded patches are inside Google Drive.</p> <pre><code># Connect to Drive\ndrive.mount('/content/drive')\n</code></pre>"},{"location":"Preprocessing%20patches/#03-reading-patches-dowloaded-from-earth-engine","title":"03. Reading patches dowloaded from Earth Engine","text":"<pre><code># prefix of patches\nfile_prefix = 'Port'\n\n# Create a path to the exported folder\npath = Path('/content/drive/MyDrive/Training_patches_512-512_Portugal/tile_17')\n\n# creating a list of patches paths\npaths = [f for f in path.iterdir() if file_prefix in f.stem]\npaths, len(paths)\n</code></pre>"},{"location":"Preprocessing%20patches/#04-display-metadata-for-patches","title":"04. Display metadata for patches","text":"<p>Patch dimensions (pixels per rows, cols), patches per row and total patches are crucial for the unpatched process.</p> <pre><code># load the mixer json\njson_file = str(path/(file_prefix+'-mixer.json'))\njson_text = !cat \"{json_file}\"\n\n# print the info\nmixer = json.loads(json_text.nlstr)\nmixer\n</code></pre> <p>Number of patches per row and col is display in the following image:</p> <p></p> <p>Getting patch dimensions an total patches:</p> <pre><code># Get relevant info from the JSON mixer file\npatch_width = mixer['patchDimensions'][0]\npatch_height = mixer['patchDimensions'][1]\npatches = mixer['totalPatches']\npatch_dimensions_flat = [patch_width, patch_height]\npatch_dimensions_flat, patches\n</code></pre>"},{"location":"Preprocessing%20patches/#05-define-the-structure-of-your-data","title":"05. Define the structure of your data","text":"<p>For parsing the exported TFRecord files, <code>featuresDict</code> is a mapping between feature names (recall that <code>featureNames</code> contains the band and label names) and <code>float32</code> <code>tf.io.FixedLenFeature</code> objects. This mapping is necessary for telling TensorFlow how to read data in a TFRecord file into tensors. Specifically, all numeric data exported from Earth Engine is exported as float32.</p> <p>Note: features in the TensorFlow context (i.e. <code>tf.train.Feature</code>) are not to be confused with Earth Engine features (i.e. <code>ee.Feature</code>), where the former is a protocol message type for serialized data input to the model and the latter is a geometry-based geographic data structure.</p> <pre><code># bands names\nbands = ['B2_median', 'B3_median', 'B4_median', 'B5_median', 'B6_median', 'B7_median', 'B8_median',\n         'B8A_median', 'B11_median', 'B12_median', 'dNBR', 'label']\n\n# list of fixed-length features, all of which are float32.\ncolumns = [tf.io.FixedLenFeature(shape=patch_dimensions_flat, dtype=tf.float32) for b in bands]\n\n# dictionary with names as keys, features as values.\nfeatures_dict = dict(zip(bands, columns))\nfeatures_dict\n</code></pre>"},{"location":"Preprocessing%20patches/#06-parse-the-dataset","title":"06. Parse the dataset","text":"<p>Now we need to make a parsing function for the data in the TFRecord files. The data comes in flattened 2D arrays per record and we want to use the first part of the array for input to the model and the last element of the array as the class label. The parsing function reads data from a serialized <code>Example</code> proto into a dictionary in which the keys are the feature names and the values are the tensors storing the value of the features for that example. (These TensorFlow docs explain more about reading Example protos from TFRecord files).</p> <pre><code># parsing function\ndef parse_tfrecord(example_proto):\n  \"\"\"The parsing function.\n\n  Read a serialized example into the structure defined by featuresDict.\n\n  Args:\n    example_proto: a serialized Example.\n\n  Returns:\n    A tuple of the predictors dictionary including the label.\n  \"\"\"\n  return tf.io.parse_single_example(example_proto, features_dict)\n</code></pre> <p>Note that you can make one dataset from many files by specifying a list</p> <pre><code>patch_dataset = tf.data.TFRecordDataset(str(path/(file_prefix+'-00008.tfrecord.gz')), compression_type='GZIP')\nds = patch_dataset.map(parse_tfrecord, num_parallel_calls = 5)\nds\n</code></pre>"},{"location":"Preprocessing%20patches/#07-patches-as-tensors","title":"07. Patches as tensors","text":"<p>In this step, patches downloaded for GEE will be saved as a tensor with shape <code>(a, b, c, d)</code>, 4D. Where <code>a</code> represents the number of patches, <code>b</code> and <code>c</code> represent the patch dimension and <code>d</code> represents the number of bands for each patch.</p> <pre><code># number of bands\nnbands = 11\n\n# empty tensors\narray_image = np.zeros((patch_width, patch_height, nbands)) # filas, columnas y n bandas\narray_label = np.zeros((patch_width, patch_height, 1)) # filas, columnas y n bandas\n\n# let's save patches in a list\nXl = [] # Sentinel-2 bands\nyl = [] # Labeling data\n\n# bands names\nbands_image = ['B2_median', 'B3_median', 'B4_median', 'B5_median', 'B6_median',\n               'B7_median', 'B8_median','B8A_median', 'B11_median', 'B12_median', 'dNBR']\nbands_label = ['label']\n\n# stacking images within a tensor\n# len(paths)-1\nfor file in range(len(paths)-1):\n  image_dataset = tf.data.TFRecordDataset(str(paths[file]), compression_type = 'GZIP')\n  ds = image_dataset.map(parse_tfrecord, num_parallel_calls = 10)\n  arr = list(ds.as_numpy_iterator())\n  # for sentinel-2 bands\n  for j in range(len(arr)):\n    names = arr[j]\n    for i, bnames in enumerate(bands_image):\n      array_image[:,:,i] = names[bnames]\n    Xl.append(array_image.copy())\n  # for labeling data\n  for j in range(len(arr)):\n    names = arr[j]\n    for i, bnames in enumerate(bands_label):\n      array_label[:,:,i] = names[bnames]\n    yl.append(array_label.copy())\n</code></pre> <p>Then, patches can be converted to <code>np.array</code>. So, in total we have 108 patches with 512x512 pixels for both image and labeling data.</p> <pre><code># list to array\nX = np.array(Xl)\ny = np.array(yl)\n\n# print basic details\nprint('Input features shape:', X.shape)\nprint('\\nInput labels shape:', y.shape)\n</code></pre> <p>Dealing with NaN values: In this step we are replacing NaN values for 0, in case our data contain <code>Null</code> values. This will allow computing matrix operations with deep learning architectures.</p> <pre><code># replacing nan -&gt; 0\nX[np.isnan(X)] = 0\n\n# replacing nan -&gt; 0\ny[np.isnan(y)] = 0\n</code></pre>"},{"location":"Preprocessing%20patches/#08-normalizing-data","title":"08. Normalizing data","text":"<p>Machine learning algorithms are often trained with the assumption that all features contribute equally to the final prediction. However, this assumption fails when the features differ in range and unit, hence affecting their importance.</p> <p>Enter normalization - a vital step in data preprocessing that ensures uniformity of the numerical magnitudes of features. This uniformity avoids the domination of features that have larger values compared to the other features or variables.</p> <p>Normalization fosters stability in the optimization process, promoting faster convergence during gradient-based training. It mitigates issues related to vanishing or exploding gradients, allowing models to reach optimal solutions more efficiently. Please see this article for more detail.</p> <p>Z-score normalization:</p> <p>Z-score normalization (standardization) assumes a Gaussian (bell curve) distribution of the data and transforms features to have a mean (\u03bc) of 0 and a standard deviation (\u03c3) of 1. The formula for standardization is:</p> <p>Xstandardized = (X - \u03bc)/ \u03c3</p> <p>This technique is particularly useful when dealing with algorithms that assume normally distributed data, such as many linear models.</p> <p>Z-score normalization is available in scikit-learn via StandardScaler.</p> <pre><code># normalizing data using scikit-learn\nfor i in range(X.shape[3]):\n    band = X[:, :, :, i]\n    band_normalized = StandardScaler().fit_transform(band.reshape(-1, 1)).reshape(108, 512, 512)\n    X[:, :, :, i] = band_normalized\n\n# Verifying dimensions (shape)\nprint(\"Normalized array:\", X.shape)\n</code></pre> <p>Labeling data only has two values 0 and 1, which 0 means unburned and 1 burned area. According to your data, labeling data could have many values.</p> <pre><code># details\nprint('Values in input features, min: %d &amp; max: %d' % (np.min(y), np.max(y)))\n</code></pre>"},{"location":"Preprocessing%20patches/#09-visualization-of-patch","title":"09. Visualization of patch","text":"<p>Visualizing patches and labeling data using the <code>scikit-eo</code> Python package with the <code>plotRGB</code> function.</p> <p><pre><code>orig_map = plt.colormaps['Greys']\nreversed_map = orig_map.reversed()\n\nfig, ax = plt.subplots(2, 2, figsize = (9,9))\n\nfor i, idx in enumerate([17,15]):\n  rgb = np.moveaxis(np.array(Xl)[idx,:,:,:], -1, 0)\n  # satellite image (patches)\n  plotRGB(rgb, bands = [10, 8, 3], title = 'Burned area: 512x512 pixels',\n           stretch = 'per', ax = ax[0, i])\n  # labeling data\n  ax[1, i].imshow(y[idx,:,:,:], cmap = reversed_map)\n  ax[1, i].get_xaxis().set_visible(True)\n  ax[1, i].get_yaxis().set_visible(True)\n</code></pre> </p>"},{"location":"Training%20and%20validation/","title":"03-Training and validation","text":""},{"location":"Training%20and%20validation/#01-libraries-to-be-installed","title":"01. Libraries to be installed:","text":"<pre><code>!pip install patchify\n</code></pre>"},{"location":"Training%20and%20validation/#02-libraries-to-be-used","title":"02. Libraries to be used:","text":"<pre><code>import random\nimport numpy as np\nfrom scikeo.plot import plotRGB\nimport matplotlib as mpl\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.metrics import confusion_matrix\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import models\nfrom keras import layers\nfrom keras.metrics import MeanIoU\nfrom keras.utils import to_categorical\nfrom keras import layers, Model\nfrom patchify import patchify, unpatchify\nfrom google.colab import drive\n</code></pre>"},{"location":"Training%20and%20validation/#03-training-and-testing","title":"03. Training and testing","text":"<p>To train Machine Learning and Deep Learning models, 80% of the data could be used and 20% is used as validation of the trained model. But, sometimes this percetange could change, e.g., 50% for training and 50% for testing or 70% for training and the rest for testing, etc.</p> <p>So, in this material, 80% will be used for traning. Getting training and testing data:</p> <pre><code>Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, train_size = 0.6, random_state = None)\n\n# visualizing the array dimensions\nXtrain.shape, Xtest.shape, ytrain.shape, ytest.shape\n</code></pre> <pre><code># unique classes\nn_classes = len(np.unique(y))\n# converts a class vector (integers) to binary class matrix\nytrain_catego = to_categorical(ytrain, num_classes = n_classes)\nytest_catego = to_categorical(ytest, num_classes = n_classes)\n\n# visualizing the shape of the arrays\nXtrain.shape, Xtest.shape, ytrain_catego.shape, ytest_catego.shape\n</code></pre>"},{"location":"Training%20and%20validation/#04-u-net-architecture","title":"04. U-net architecture","text":"<p>To import or implement the U-Net model in Python, you usually need to define the architecture manually since U-Net is not part of the predefined architectures in libraries like TensorFlow or Keras. You can implement U-Net from scratch or use available third-party implementations.</p> <p>Here is a basic example of how to define the U-Net architecture using Keras (with TensorFlow backend):</p> <p>The following codes define a typical U-Net model with:</p> <ul> <li>An encoding path (encoder) that reduces the dimensions using <code>Conv2D</code> and <code>MaxPooling2D</code>.</li> <li>A decoding path (decoder) that restores the original dimensions using <code>Conv2DTranspose</code>.</li> <li>Skip connections between the encoder and decoder layers to retain spatial information.</li> </ul> <pre><code># U-net architecture\ndef unet_model(num_classes = 2, img_height = 256, img_width = 256, img_channels = 1):\n    # Build the model\n    inputs = layers.Input((img_height, img_width, img_channels))\n\n    # Down-sampling path (Encoder): Contraction path\n    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n    conv1 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv1)\n    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n\n    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(pool1)\n    conv2 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv2)\n    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n\n    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(pool2)\n    conv3 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv3)\n    pool3 = layers.MaxPooling2D(pool_size=(2, 2))(conv3)\n\n    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(pool3)\n    conv4 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv4)\n    pool4 = layers.MaxPooling2D(pool_size=(2, 2))(conv4)\n\n    # Bottleneck\n    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(pool4)\n    conv5 = layers.Conv2D(1024, 3, activation='relu', padding='same')(conv5)\n\n    # Up-sampling path (Decoder): Expansive path\n    up6 = layers.Conv2DTranspose(512, 2, strides=(2, 2), padding='same')(conv5)\n    up6 = layers.concatenate([up6, conv4])\n    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(up6)\n    conv6 = layers.Conv2D(512, 3, activation='relu', padding='same')(conv6)\n\n    up7 = layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(conv6)\n    up7 = layers.concatenate([up7, conv3])\n    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(up7)\n    conv7 = layers.Conv2D(256, 3, activation='relu', padding='same')(conv7)\n\n    up8 = layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(conv7)\n    up8 = layers.concatenate([up8, conv2])\n    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(up8)\n    conv8 = layers.Conv2D(128, 3, activation='relu', padding='same')(conv8)\n\n    up9 = layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(conv8)\n    up9 = layers.concatenate([up9, conv1])\n    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(up9)\n    conv9 = layers.Conv2D(64, 3, activation='relu', padding='same')(conv9)\n\n    # Output layer\n    if num_classes == 1:\n        outputs = layers.Conv2D(1, 1, activation='sigmoid')(conv9)  # Binary classification\n    else:\n        outputs = layers.Conv2D(num_classes, 1, activation='softmax')(conv9)  # Multi-class classification\n\n    model = Model(inputs=[inputs], outputs=[outputs])\n\n    return model\n</code></pre> <p>Keep in mind:</p> <ol> <li><code>num_classes</code> argument: This argument controls the number of output classes.</li> <li>If <code>num_classes=1</code>, the model will use <code>sigmoid</code> activation for binary classification.</li> <li>If <code>num_classes&gt;1</code>, the model will use <code>softmax</code> activation for multi-class classification.</li> <li>Output Layer:</li> <li>For binary classification: <code>Conv2D(1, 1, activation='sigmoid')</code>.</li> <li>For multi-class classification: <code>Conv2D(num_classes, 1, activation='softmax')</code>.</li> </ol> <p>Now, you can use the <code>num_classes</code> argument to create either a binary classification U-Net or a multi-class classification U-Net.</p> <pre><code># calling the model\nmodel = unet_model(num_classes = 2, img_height = 512, img_width = 512, img_channels = 11)\n#model.compile(optimizer='adam', loss=total_loss, metrics=metrics)\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()\n</code></pre> <p>Verifying the input shape:</p> <pre><code>model.input_shape\n</code></pre>"},{"location":"Training%20and%20validation/#05-running-the-trained-model","title":"05. Running the trained model","text":"<p>You'll now train the model for 220 epochs (i.e., 220 iterations over all samples in the <code>Xtrain</code> and <code>ytrain_catego</code> tensors), in mini-batches of 8 samples.</p> <p>Hyperparameters used: - Number of epochs: <code>epochs = 220</code> - Batch size: <code>batch_size = 8</code></p> <p>These hyperparameters can be modify according to your needs and computational capabilities.</p> <pre><code># Run the model\nhistory = model.fit(Xtrain,\n                    ytrain_catego,\n                    epochs = 220,\n                    batch_size = 8,\n                    verbose = 1)\n</code></pre> <p>Important!: How can we improve the accuracy?, There are some hyperparameters that you can modify in order to increase the accuracy of the model. - For example, <code>epochs</code>: Increasing the number of iterations is often a practice to improve the accuracy.</p> <p>However, not always increasing this hyperparameter will lead to better accuracy or prediction of the models. It is important to find a balance to what extent it can affect on the final performance of our DL model. Do not go for overfitting or underfitting.</p>"},{"location":"Training%20and%20validation/#06-monitoring-during-training","title":"06. Monitoring during training","text":"<p>In order to monitor during training the accuracy of the model on data it has never seen before, you'll create a validation set by setting apart of n% samples from the original training data. You'll monitor <code>loss</code> and <code>accuracy</code> on the n% samples that you set apart. You do so by passing the validation data as the <code>validation_data</code> argument as follow:</p> <pre><code># 10 percent for validation\nhistory = model.fit(Xtrain,\n                    ytrain_catego,\n                    epochs = 220,\n                    batch_size = 8,\n                    validation_data = 0.1,\n                    verbose = 1)\n</code></pre> <p>If you want to avoid this argument during training, you'll monitor loss and accuracy with the <code>Xtrain</code> and <code>ytrain_catego</code> tensors.</p> <pre><code>hist = history.history\n\nepochs = range(220)\n\nfig, axes = plt.subplots(figsize = (6,5))\nln1 = axes.plot(epochs, hist.get('accuracy'), marker ='o', markersize = 6,  label = 'Training acc')\n\naxes2 = axes.twinx()\nln2 = axes2.plot(epochs, hist.get('loss'), marker = 'o', color = 'r', label = \"Training loss\")\n\n# lengend\nln = ln1 + ln2\nlabs = [l.get_label() for l in ln]\n\naxes.legend(ln, labs, loc = 'center right')\naxes.set_xlabel(\"Num of epochs\")\naxes.set_ylabel(\"OA\")\naxes2.set_ylabel(\"Loss values\")\naxes2.grid(False)\nplt.show()\n</code></pre> <p>Print the accuracy obtained on data it has never seen before with <code>Xtest</code> and <code>ytest_catego</code>:</p> <pre><code>test_loss, test_acc = model.evaluate(Xtest, ytest_catego)\nprint(f'test_loss: {test_loss}\\ntest_acc: {test_acc}')\n</code></pre>"},{"location":"Training%20and%20validation/#07-saving-and-reading-the-model-trained","title":"07. Saving and reading the model trained","text":"<p>The section below illustrates how to save and restore the model in the <code>.keras</code> format. - Please see this website https://www.tensorflow.org/tutorials/keras/save_and_load for more details.</p> <pre><code># Save the entire model as a `.keras` zip archive.\nmodel.save('/content/drive/MyDrive/00_PhD/models/Portugal/unet_model_6tiles.keras')\n</code></pre> <p>Reload a fresh Keras model from the <code>.keras</code> zip archive:</p> <pre><code># Reading the model saved\nmodel = tf.keras.models.load_model('/content/drive/MyDrive/00_PhD/models/Portugal/unet_model_6tiles.keras')\n\n# Show the model architecture\nmodel.summary()\n</code></pre> <p>Now you can verify the expected dimensions for input</p> <pre><code># expected dimensions for input\nmodel.input_shape\n</code></pre>"},{"location":"Training%20and%20validation/#08-accuracy-assessment-of-burned-area-mapping","title":"08. Accuracy assessment of burned area mapping","text":"<p>To evaluate the accuracy of classifications map, indicators such as overall accuracy (this was obtained from the error matrix), Recall, Precision, F1-score and Intersection-Over-Union were used.</p> <pre><code># define classes\nclass_names = ['burned', 'unburned']\n\n# compute predictions\ny_pred = model.predict(Xtest)\ny_pred_argmax = np.argmax(y_pred, axis = 3)\ny_test_argmax = np.argmax(ytest_catego, axis = 3)\n</code></pre> <ol> <li>Overall Accuracy</li> </ol> <pre><code># Overall accuracy obtained\nprint(\"overall accuracy: {:.4f}\".format(accuracy_score(y_test_argmax.flatten(), y_pred_argmax.flatten())))\n</code></pre> <ol> <li>Recall</li> </ol> <pre><code># Recall obtained\nprint(\"Recall obtained: {:.4f}\".format(recall_score(y_test_argmax.flatten(), y_pred_argmax.flatten())))\n</code></pre> <ol> <li>Precision</li> </ol> <pre><code># Precision obtained\nprint(\"Precision obtained: {:.4f}\".format(precision_score(y_test_argmax.flatten(), y_pred_argmax.flatten())))\n</code></pre> <ol> <li>F1-score</li> </ol> <pre><code># F1-score obtained\nprint(\"F1-score obtained: {:.4f}\".format(f1_score(y_test_argmax.flatten(), y_pred_argmax.flatten())))\n</code></pre> <ol> <li>IoU (Intersection-Over-Union)</li> </ol> <p>It is a common evaluation metric for semantic image segmentation. How does it work?</p> <p>confusion matrix = [(1, 1), (1, 1)]</p> <p>sum_row = (2, 2), sum_col = (2, 2), true_positives = (1, 1)</p> <p>iou = true_positives/(sum_row + sum_col - true_positives)</p> <p>iou = [0.33, 0.33]</p> <ul> <li>https://www.tensorflow.org/api_docs/python/tf/keras/metrics/IoU</li> </ul> <pre><code># Using built in keras function for IoU\nn_classes = len(np.unique(y))\nIOU_keras = MeanIoU(num_classes=n_classes)\nIOU_keras.update_state(y_test_argmax, y_pred_argmax)\nprint(\"Mean IoU =\", IOU_keras.result().numpy())\n</code></pre>"},{"location":"Training%20and%20validation/#09-confusion-matrix","title":"09. Confusion matrix","text":"<p>Let's plot Confusion Matriz</p> <pre><code>def plotConfusionMatrix(y_test , y_pred , text_size = 12, classes = None , ax = None):\n\n  if ax is None:\n        ax = plt.gca() # get current axes\n\n  # Create the confusion matrix from sklearn\n  cm = confusion_matrix(y_test , y_pred)\n  threshold = (cm.max() + cm.min()) /2\n\n  # Number of clases\n  n_classes = cm.shape[0]\n\n  # Drawing the matrix plot\n  b = ax.matshow(cm , cmap = plt.cm.Blues)\n  bar = plt.colorbar(b)\n\n  # Set labels to be classes\n  if classes:\n    labels = classes\n  else:\n    labels = np.arange(cm.shape[0])\n\n  # Label axes\n  ax.set(title ='Confusion Matrix', xlabel = 'Ground-truth' ,\n         ylabel = 'Prediction', xticks = np.arange(n_classes),\n         yticks = range(n_classes), xticklabels = labels , yticklabels = labels)\n  # Set the xaxis labels to bottom\n  ax.xaxis.set_label_position('bottom')\n  ax.xaxis.tick_bottom()\n\n  # Adjust the label size\n  ax.yaxis.label.set_size(text_size)\n  ax.xaxis.label.set_size(text_size)\n  ax.title.set_size(text_size)\n  ax.grid(False)\n\n  # Plot the text on each cell\n  for i in range(cm.shape[0]):\n    for j in range(cm.shape[1]):\n      ax.text(i,j, f'{cm[i,j]}',\n               horizontalalignment = 'center' ,\n               color = 'white' if cm[i , j] &gt; threshold else 'black',\n               size = text_size)\n</code></pre> <pre><code>fig, ax = plt.subplots(figsize = (8,6))\nplotConfusionMatrix(y_test_argmax.flatten(), y_pred_argmax.flatten(), classes = class_names)\n</code></pre> <p></p>"},{"location":"Training%20and%20validation/#10-predicting-for-one-patch","title":"10. Predicting for one patch","text":"<pre><code>test_img_number = random.randint(17, len(Xtest))\ntest_img = Xtest[test_img_number]\nground_truth = y_test_argmax[test_img_number]\ntest_img_input=np.expand_dims(test_img, 0)\nprediction = (model.predict(test_img_input))\npredicted_img=np.argmax(prediction, axis=3)[0,:,:]\n</code></pre> <pre><code># histogram with percentiles\ndef hist_percentile(arr_rgb):\n  p10 = np.nanpercentile(arr_rgb, 10) # percentile10\n  p90 = np.nanpercentile(arr_rgb, 90) # percentile90\n  clipped_arr = np.clip(arr_rgb, p10, p90)\n  arr_rgb_norm = (clipped_arr - p10)/(p90 - p10)\n  return arr_rgb_norm\n\norig_map = plt.colormaps['Greys']\nreversed_map = orig_map.reversed()\n\nfig, ax = plt.subplots(1, 3, figsize = (12,8))\n\nrgb_patch = hist_percentile(np.dstack((test_img[:,:,9], test_img[:,:,6], test_img[:,:,2])))\nax[0].imshow(rgb_patch)\nax[0].set_title('Sentinel-2 patch (512, 512)')\nax[1].imshow(ground_truth, cmap = reversed_map)\nax[1].set_title('Testing Label')\nax[2].imshow(predicted_img, cmap = reversed_map)\nax[2].set_title('Prediction on test image')\nplt.show()\n</code></pre>"}]}